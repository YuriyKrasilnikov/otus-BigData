{
  "metadata": {
    "name": "HomeWork3-Spark-Streaming-Pipeline-2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sc.version"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.{Transformer, Estimator, Model, Pipeline, PipelineModel}\r\nimport org.apache.spark.ml.param.{Params, ParamMap, Param, DoubleParam, StringArrayParam}\r\nimport org.apache.spark.ml.util.{Identifiable, DefaultParamsReadable, DefaultParamsWritable}\r\nimport org.apache.spark.ml.feature.{StringIndexer, Imputer, VectorAssembler, Interaction, StandardScaler}\r\nimport org.apache.spark.ml.linalg.{Vector, Vectors, DenseVector}\r\nimport org.apache.spark.ml.classification.{LogisticRegression}\r\nimport org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator}\r\nimport org.apache.spark.sql.{SQLContext, DataFrame, Dataset, Encoder, Encoders}\r\nimport org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.sql.functions.{col, lit, explode, udf}\r\nimport org.apache.spark.sql.types.{DoubleType, DataTypes, StructType, StructField}\r\nimport scala.collection.mutable.{ArrayBuffer}"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Common Parameters\r\ntrait NullColumnParams extends Params {\r\n    // Defining a Param requires 3 elements:\r\n    //  - Param definition\r\n    //  - Param getter method\r\n    //  - Param setter method\r\n    \r\n    /**---\r\n        * Param for input column names.\r\n    */\r\n    final val inputCols: StringArrayParam \u003d new StringArrayParam(this, \"inputCols\", \"Input column names\")\r\n    \r\n    final def getInputCols: Array[String] \u003d $(inputCols)\r\n    \r\n    final def setInputCols(value: Array[String]): this.type \u003d set(inputCols, value)\r\n    \r\n    setDefault(inputCols -\u003e Array.empty[String])\r\n    \r\n    \r\n    final val outputCols: StringArrayParam \u003d new StringArrayParam(this, \"outputCols\", \"Output column names\")\r\n    \r\n    final def getOutputCols: Array[String] \u003d $(outputCols)\r\n    \r\n    final def setOutputCols(value: Array[String]): this.type \u003d set(outputCols, value)\r\n    \r\n    setDefault(outputCols -\u003e Array.empty[String])\r\n    \r\n    \r\n    final val missingValue: DoubleParam \u003d new DoubleParam(this, \"missingValue\", \"Replace null values\")\r\n    \r\n    final def getMissingValue: Double \u003d $(missingValue)\r\n    \r\n    final def setMissingValue(value: Double): this.type \u003d set(missingValue, value)\r\n    \r\n    setDefault(missingValue -\u003e -1.0)\r\n \r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "class NullColumnModel(override val uid: String) extends Model[NullColumnModel]\n    with DefaultParamsWritable // Enables Serialization\n    with NullColumnParams {\n    \n    import spark.implicits._\n    \n    \n    // Transformer requires 3 methods:\n    //  - transform\n    //  - transformSchema\n    //  - copy\n\n    \n    /**\n    * This method implements the main transformation.\n    * Its required semantics are fully defined by the method API: take a Dataset or DataFrame,\n    * and return a DataFrame.\n    */\n    override def transform(dataset: Dataset[_]): DataFrame \u003d {\n        val colsMap\u003d($(inputCols) zip $(outputCols)).toMap\n\n        colsMap.foldLeft(dataset.toDF)(\n            (ds, columnMap) \u003d\u003e ds.withColumn(columnMap._2, col(columnMap._1) )\n        ).na.fill($(missingValue), $(outputCols))\n    }\n    \n    /*\n    * Check transform validity and derive the output schema from the input schema.\n    *\n    * We check validity for interactions between parameters during `transformSchema` and\n    * raise an exception if any parameter value is invalid. Parameter value checks which\n    * do not depend on other parameters are handled by `Param.validate()`.\n    *\n    * Typical implementation should first conduct verification on schema change and parameter\n    * validity, including complex parameter interaction checks.\n    */\n    \n    override def transformSchema(schema: StructType): StructType \u003d {\n        // Validate input type.\n        // Input type validation is technically optional, but it is a good practice since it catches\n        // schema errors early on.\n        $(inputCols).foreach((inputColName: String) \u003d\u003e {\n            require(schema.fieldNames.contains(inputColName), s\"Input column $inputColName must exist.\")\n        })\n        \n        StructType(\n            schema.fields ++ $(outputCols).map(c\u003d\u003e{StructField(c,DoubleType,false)})\n        )\n    }\n    /**\n    * Creates a copy of this instance.\n    * Requirements:\n    *  - The copy must have the same UID.\n    *  - The copy must have the same Params, with some possibly overwritten by the `extra`\n    *    argument.\n    *  - This should do a deep copy of any data members which are mutable.  That said,\n    *    Transformers should generally be immutable (except for Params), so the `defaultCopy`\n    *    method often suffices.\n    * @param extra  Param values which will overwrite Params in the copy.\n    */\n    override def copy(extra: ParamMap): NullColumnModel \u003d defaultCopy(extra)\n}\n\nobject NullColumnModel extends DefaultParamsReadable[NullColumnModel]"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "class NullColumnEncoder(override val uid: String) extends Estimator[NullColumnModel]\n    with DefaultParamsWritable // Enables Serialization of MyCommonParams\n    with NullColumnParams {\n    \n    import spark.implicits._\n\n    def this() \u003d this(Identifiable.randomUID(\"NullColumnEncoder\"))\n\n    /**\n    * Creates a copy of this instance.\n    * Requirements:\n    *  - The copy must have the same UID.\n    *  - The copy must have the same Params, with some possibly overwritten by the `extra`\n    *    argument.\n    *  - This should do a deep copy of any data members which are mutable.  That said,\n    *    Transformers should generally be immutable (except for Params), so the `defaultCopy`\n    *    method often suffices.\n    * @param extra  Param values which will overwrite Params in the copy.\n    */\n    override def copy(extra: ParamMap): Estimator[NullColumnModel] \u003d defaultCopy(extra)\n\n    \n    override def transformSchema(schema: StructType): StructType \u003d {\n        // Validate input type.\n        // Input type validation is technically optional, but it is a good practice since it catches\n        // schema errors early on.\n        $(inputCols).foreach((inputColName: String) \u003d\u003e {\n            require(schema.fieldNames.contains(inputColName), s\"Input column $inputColName must exist.\")\n        })\n        \n        StructType(\n            schema.fields\n        )\n    }\n    \n    override def fit(dataset: Dataset[_]): NullColumnModel \u003d {\n        copyValues(\n            new NullColumnModel(\n                uid + \"_model\"\n            ).setParent(this)\n        )\n    }\n}\n\n// Companion object enables deserialization of MyCommonParams\nobject NullColumnEncoder extends DefaultParamsReadable[NullColumnEncoder]"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Common Parameters\r\ntrait ArrayOneHotEncoderParams extends Params {\r\n    // Defining a Param requires 3 elements:\r\n    //  - Param definition\r\n    //  - Param getter method\r\n    //  - Param setter method\r\n    \r\n    /**---\r\n        * Param for input column names.\r\n    */\r\n    final val inputCols: StringArrayParam \u003d new StringArrayParam(this, \"inputCols\", \"Input column names\")\r\n    \r\n    final def getInputCols: Array[String] \u003d $(inputCols)\r\n    \r\n    final def setInputCols(value: Array[String]): this.type \u003d set(inputCols, value)\r\n    \r\n    setDefault(inputCols -\u003e Array.empty[String])\r\n    \r\n    /**---\r\n        * Param for output column names.\r\n    */\r\n    final val outputCols: StringArrayParam \u003d new StringArrayParam(this, \"outputCols\", \"Output column names\")\r\n    \r\n    final def getOutputCols: Array[String] \u003d $(outputCols)\r\n    \r\n    final def setOutputCols(value: Array[String]): this.type \u003d set(outputCols, value)\r\n    \r\n    setDefault(outputCols -\u003e Array.empty[String])\r\n    \r\n\r\n    /**---\r\n        * The scheme of the columns, if know. Otherwise it is created whan fit\r\n    */\r\n\r\n    final val knownCols: StringArrayParam \u003d new StringArrayParam(this, \"knownCols\", \"The scheme of the columns, if know(arrays of string like know_column_name:know_column_value)\")\r\n    \r\n    final def getKnownCols: Array[String] \u003d $(knownCols)\r\n    \r\n    final def setKnownCols(value: Array[String]): this.type \u003d set(knownCols, value)\r\n    \r\n    setDefault(knownCols -\u003e Array.empty[String])\r\n\r\n    \r\n    /**---\r\n       * The name of the columns for which you want to create new columns\r\n    */\r\n    \r\n    final val createColsByValues: StringArrayParam \u003d new StringArrayParam(this, \"createColsByValues\", \"The name of the columns for which you want to create new columns\")\r\n    \r\n    final def getCreateColsByValues: Array[String] \u003d $(createColsByValues)\r\n    \r\n    final def setCreateColsByValues(value: Array[String]): this.type \u003d set(createColsByValues, value)\r\n    \r\n    setDefault(createColsByValues -\u003e Array.empty[String])\r\n    \r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "class ArrayOneHotEncoderModel(override val uid: String, knowColsMap: Map[String, Array[String]]) extends Model[ArrayOneHotEncoderModel]\n    with DefaultParamsWritable // Enables Serialization\n    with ArrayOneHotEncoderParams {\n    \n    import spark.implicits._\n    \n    //---\n    private var newCols: ArrayBuffer[StructField] \u003d ArrayBuffer.empty[StructField]\n    \n    \n    // my functions\n    \n            \n    private def bool2double(b:Boolean) \u003d if (b) 1.0 else 0.0\n    \n    private def arrayToCol(df:DataFrame, inputColumn:String, outputColumn:String, arrVal:Array[String]): DataFrame \u003d {\n        if($(createColsByValues).contains(inputColumn)){\n            arrVal.foldLeft(df){\n                case (acc, arr) \u003d\u003e {\n                    val columnName:String \u003d outputColumn+\"_\"+arr\n                    newCols+\u003dStructField(columnName, DoubleType, true) \n                    acc.withColumn(columnName, array_contains(col(inputColumn),arr).cast(DoubleType))\n                }\n            }\n        }else{\n            val containArray \u003d udf[Vector, Array[String]](arr\u003d\u003e{\n                                                                    Vectors.dense(\n                                                                        arrVal.map(c\u003d\u003e{\n                                                                            bool2double(arr.contains(c))\n                                                                        })\n                                                                    )\n                                                                })\n            newCols+\u003dStructField(outputColumn, DoubleType, true) \n            df.withColumn(\n                outputColumn,\n                containArray( col(inputColumn) )\n            )\n        }\n    }\n    \n    private def getArgName(df:DataFrame, colName:String): Array[String] \u003d {\n        if (knowColsMap.contains(colName)){\n            knowColsMap(colName)\n        }else{\n            Array.empty[String]\n        }\n    }\n    \n    \n    // Transformer requires 3 methods:\n    //  - transform\n    //  - transformSchema\n    //  - copy\n\n    \n    /**\n    * This method implements the main transformation.\n    * Its required semantics are fully defined by the method API: take a Dataset or DataFrame,\n    * and return a DataFrame.\n    */\n    override def transform(dataset: Dataset[_]): DataFrame \u003d {\n        val colsMap\u003d($(inputCols) zip $(outputCols)).toMap\n\n        colsMap.foldLeft(dataset.toDF){\n            case (acc, columnMap) \u003d\u003e {\n                val argName: Array[String] \u003d getArgName(df\u003dacc,colName\u003dcolumnMap._1)\n                arrayToCol(df\u003dacc, inputColumn\u003dcolumnMap._1, outputColumn\u003dcolumnMap._2, arrVal\u003dargName)\n            }\n        }\n    }\n    \n    /*\n    * Check transform validity and derive the output schema from the input schema.\n    *\n    * We check validity for interactions between parameters during `transformSchema` and\n    * raise an exception if any parameter value is invalid. Parameter value checks which\n    * do not depend on other parameters are handled by `Param.validate()`.\n    *\n    * Typical implementation should first conduct verification on schema change and parameter\n    * validity, including complex parameter interaction checks.\n    */\n    \n    override def transformSchema(schema: StructType): StructType \u003d {\n        // Validate input type.\n        // Input type validation is technically optional, but it is a good practice since it catches\n        // schema errors early on.\n        $(inputCols).foreach((inputColName: String) \u003d\u003e {\n            require(schema.fieldNames.contains(inputColName), s\"Input column $inputColName must exist.\")\n        })\n        \n        StructType(\n            schema.fields ++ newCols\n        )\n    }\n    /**\n    * Creates a copy of this instance.\n    * Requirements:\n    *  - The copy must have the same UID.\n    *  - The copy must have the same Params, with some possibly overwritten by the `extra`\n    *    argument.\n    *  - This should do a deep copy of any data members which are mutable.  That said,\n    *    Transformers should generally be immutable (except for Params), so the `defaultCopy`\n    *    method often suffices.\n    * @param extra  Param values which will overwrite Params in the copy.\n    */\n    override def copy(extra: ParamMap): ArrayOneHotEncoderModel \u003d defaultCopy(extra)\n}\n\nobject ArrayOneHotEncoderModel extends DefaultParamsReadable[ArrayOneHotEncoderModel]"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "class ArrayOneHotEncoder(override val uid: String) extends Estimator[ArrayOneHotEncoderModel]\n    with DefaultParamsWritable // Enables Serialization of MyCommonParams\n    with ArrayOneHotEncoderParams {\n    \n    import spark.implicits._\n\n    def this() \u003d this(Identifiable.randomUID(\"OneHotEncoderExtended\"))\n    \n    private def knownColsMapParser(date: Array[String]): Map[String, Array[String]] \u003d {\n        date.foldLeft(Map.empty[String, Array[String]]){\n            case (mp, arr)\u003d\u003e{\n                val pair \u003d arr.split(\":\")\n                if(mp.contains(pair(0))){\n                    mp ++ Map(pair(0)-\u003e(mp(pair(0)):+pair(1)))\n                }else{\n                    mp ++ Map(pair(0)-\u003eArray(pair(1)))\n                }\n            }\n        }\n    }\n    \n\n    /**\n    * Creates a copy of this instance.\n    * Requirements:\n    *  - The copy must have the same UID.\n    *  - The copy must have the same Params, with some possibly overwritten by the `extra`\n    *    argument.\n    *  - This should do a deep copy of any data members which are mutable.  That said,\n    *    Transformers should generally be immutable (except for Params), so the `defaultCopy`\n    *    method often suffices.\n    * @param extra  Param values which will overwrite Params in the copy.\n    */\n    override def copy(extra: ParamMap): Estimator[ArrayOneHotEncoderModel] \u003d defaultCopy(extra)\n\n    override def transformSchema(schema: StructType): StructType \u003d {\n        // Validate input type.\n        // Input type validation is technically optional, but it is a good practice since it catches\n        // schema errors early on.\n        $(inputCols).foreach((inputColName: String) \u003d\u003e {\n            require(schema.fieldNames.contains(inputColName), s\"Input column $inputColName must exist.\")\n        })\n        \n        StructType(\n            schema.fields\n        )\n    }\n    \n    override def fit(dataset: Dataset[_]): ArrayOneHotEncoderModel \u003d {\n        \n        val knowColsMap: Map[String, Array[String]] \u003d $(inputCols)\n            .foldLeft(\n                knownColsMapParser($(knownCols))\n            ){ case (mp, column)\u003d\u003e{\n                    if(!(mp.contains(column))){\n                        val argName: Array[String] \u003d dataset.select(explode(col(column)))\n                                                .dropDuplicates()\n                                                .as[String]\n                                                .collect \n                        mp ++ Map(column-\u003eargName)\n                    }else{\n                        mp\n                    }\n                }\n            }\n\n        copyValues(\n            new ArrayOneHotEncoderModel(\n                uid + \"_model\",\n                knowColsMap\n            ).setParent(this)\n        )\n    }\n}\n// Companion object enables deserialization of MyCommonParams\nobject ArrayOneHotEncoder extends DefaultParamsReadable[ArrayOneHotEncoder]"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Read date\nval dataDir \u003d \"./notebook/otus/homework/data/train\"\nval dfRaw \u003d spark.read.load(dataDir)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create constant\n\nval label \u003d \"Liked\"\nval arrayLabelColumn \u003d \"feedback\"\n\nval labelColumn \u003d arrayLabelColumn+\"_\"+label\n\nval nullTolerance \u003d 1\nval defaultMissingValue \u003d -1.0\n\nval strategyMissingValue \u003d \"mean\"\n\nval modelPath \u003d \"./notebook/otus/tmp/spark-logistic-regression-model\"\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Allocation of columns for training"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Allocation of columns by type\n\nval Pattern \u003d \"(^Array.*)\".r\n\nval colsNumber  \u003d ArrayBuffer[String]()\nval colsString  \u003d ArrayBuffer[String]()\nval colsArray   \u003d ArrayBuffer[String]()\nval colsDate    \u003d ArrayBuffer[String]()\nval colsUnknown \u003d ArrayBuffer[String]()\n\ndfRaw\n    .dtypes\n    .foreach(ct\u003d\u003e{\n        val (c,t) \u003d ct\n        t match {\n            case \"IntegerType\"| \"LongType\" | \"DoubleType\" \u003d\u003e colsNumber+\u003dc\n            case \"StringType\"                             \u003d\u003e colsString+\u003dc\n            case \"DateType\"                               \u003d\u003e colsDate+\u003dc\n            case Pattern(_)                               \u003d\u003e colsArray+\u003dc\n            case _                                        \u003d\u003e colsUnknown+\u003dc\n        }\n    })"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def getNotFullNullColumns(dataset: Dataset[_], inputCols: Array[String], nullTolerance: Int) \u003d {        \r\n    val rowNotNull \u003d dataset.select(\r\n        inputCols.map( c \u003d\u003e { sum( col(c).isNotNull.cast(\"long\") ) }.as(c) ):_*\r\n    ).collect.apply(0)\r\n    \r\n    val mapNotNull \u003d rowNotNull.getValuesMap[Long](rowNotNull.schema.fieldNames)\r\n    \r\n    mapNotNull.filter { case (key, value) \u003d\u003e value\u003cnullTolerance }.keys.toArray\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Allocation of columns by Number completely null or not\n\nval colsNumberFullNull \u003d getNotFullNullColumns(\n    dataset\u003ddfRaw,\n    inputCols\u003dcolsNumber.toArray,\n    nullTolerance\u003dnullTolerance\n)\nval colsNumberNotFullNull \u003d colsNumber -- colsNumberFullNull"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// created cols names for VectorAssembler\nval outputColsStringIndexed     \u003d colsString.map(_+\"_indexed\").toArray\nval outputColsFullNull          \u003d colsNumberFullNull.map(_+\"_null\").toArray\nval outputColsNumberImputedMean \u003d colsNumberNotFullNull.map(_+\"_imputedMean\").toArray\nval outputColsArray             \u003d (colsArray - arrayLabelColumn).map(_+\"_oh\").toArray\n\nval inputColsVA \u003d outputColsStringIndexed ++ outputColsFullNull ++ outputColsNumberImputedMean ++ outputColsArray"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Create Label"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfFit \u003d new ArrayOneHotEncoder()\n    .setInputCols(Array(arrayLabelColumn))\n    .setOutputCols(Array(arrayLabelColumn))\n    .setKnownCols(\n        Array(\n            arrayLabelColumn+\":\"+label\n        )\n    ).setCreateColsByValues(\n        Array(arrayLabelColumn)\n    ).fit(dfRaw)\n    .transform(dfRaw)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(dfFit)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def balanceDataFrame(df: DataFrame, labelName:String, weightName:String): DataFrame \u003d {\r\n\r\n    val numNegatives \u003d df.filter(col(labelName) \u003d\u003d\u003d 0).count\r\n    val dfSize \u003d df.count\r\n    val balancingRatio \u003d (dfSize - numNegatives).toDouble / dfSize\r\n\r\n    val calculateWeights \u003d udf { d: Double \u003d\u003e\r\n      if (d \u003d\u003d 0.0) {\r\n        1 * balancingRatio\r\n      }\r\n      else {\r\n        (1 * (1.0 - balancingRatio))\r\n      }\r\n    }\r\n\r\n    df.withColumn(weightName, calculateWeights(col(labelName)))\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfBalanced \u003d balanceDataFrame(dfFit, labelColumn, \"classWeightCol\")"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(dfBalanced)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Create and fit pipeline"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val train \u003d dfBalanced"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val strategyNumberImputer \u003d new Imputer()\n  .setInputCols(colsNumberNotFullNull.toArray)\n  .setOutputCols(outputColsNumberImputedMean)\n  .setStrategy(strategyMissingValue)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val nullColumnEncoder \u003d new NullColumnEncoder()\n    .setInputCols(colsNumberFullNull.toArray)\n    .setOutputCols(outputColsFullNull)\n    .setMissingValue(defaultMissingValue)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val stringIndexer \u003d new StringIndexer()\n    .setInputCols(colsString.toArray)\n    .setOutputCols(outputColsStringIndexed)\n    .setHandleInvalid(\"keep\")"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val arrayColumnsEncoder \u003d new ArrayOneHotEncoder()\n    .setInputCols(\n        (colsArray - arrayLabelColumn).toArray\n    ).setOutputCols(outputColsArray)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pipelineFeatureTransform \u003d new Pipeline()\r\n    .setStages(\r\n        Array(\r\n            nullColumnEncoder,\r\n            strategyNumberImputer,\r\n            arrayColumnsEncoder,\r\n            stringIndexer\r\n        )\r\n    )"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val vectorAssembler \u003d new VectorAssembler()\n    .setInputCols(inputColsVA)\n    .setOutputCol(\"features\")"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val scaler \u003d new StandardScaler()\n    .setInputCol(\"features\")\n    .setOutputCol(\"scaledFeatures\")\n    .setWithStd(true)\n    .setWithMean(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pipelineFeatureCreate \u003d new Pipeline()\r\n    .setStages(\r\n        Array(\r\n            pipelineFeatureTransform.fit(train),\r\n            vectorAssembler,\r\n            scaler\r\n        )\r\n    )"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lr \u003d new LogisticRegression()\r\n    .setMaxIter(10)\r\n    .setRegParam(0.09)\r\n    .setElasticNetParam(0.4)\r\n    .setFeaturesCol(\"scaledFeatures\")\r\n    .setLabelCol(labelColumn)\r\n    .setWeightCol(\"classWeightCol\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pipeline \u003d new Pipeline()\r\n    .setStages(\r\n        Array(\r\n            pipelineFeatureCreate.fit(train),\r\n            lr\r\n        )\r\n    )"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val model \u003d pipeline.fit(train)"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//save model\n//model.write.overwrite().save(modelPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//load model\n//val model \u003d PipelineModel.load(modelPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val predictions \u003d model.transform(dfFit)"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val evaluator \u003d new BinaryClassificationEvaluator()\r\n  .setLabelCol(labelColumn)\r\n  .setRawPredictionCol(\"prediction\")\r\n  .setMetricName(\"areaUnderROC\")"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val accuracy \u003d evaluator.evaluate(predictions)\r\nprintln(s\"Accuracy: ${accuracy}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Create test stream"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.execution.streaming.{MemoryStream, Sink}\nimport org.apache.spark.sql.streaming.{StreamingQuery, OutputMode}\nimport org.apache.spark.sql.sources.{StreamSinkProvider}\nimport org.apache.spark.sql.{Row, ForeachWriter}"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Read test date\nval testDir \u003d \"./notebook/otus/homework/data/test\"\nval dfTest \u003d spark.read.load(testDir)"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.types._\r\n\r\nclass Schema2CaseClass {\r\n  type TypeConverter \u003d (DataType) \u003d\u003e String\r\n\r\n  def schemaToCaseClass(schema:StructType, className:String)(implicit tc:TypeConverter):String \u003d {\r\n    def genField(s:StructField):String \u003d {\r\n      val f \u003d tc(s.dataType)\r\n      s match {\r\n        case x if(x.nullable) \u003d\u003e s\"  ${s.name}:Option[$f]\"\r\n        case _ \u003d\u003e s\"  ${s.name}:$f\"\r\n      }\r\n    }\r\n\r\n    val fieldsStr \u003d schema.map(genField).mkString(\",\\n  \")\r\n    \r\n    s\"\"\"\r\n       |case class $className (\r\n       |  $fieldsStr\r\n       |)\r\n    \"\"\".stripMargin\r\n\r\n\r\n  }\r\n\r\n  object implicits {\r\n    implicit val defaultTypeConverter:TypeConverter \u003d (t:DataType) \u003d\u003e { t match {\r\n      case _:ByteType \u003d\u003e \"Byte\"\r\n      case _:ShortType \u003d\u003e \"Short\"\r\n      case _:IntegerType \u003d\u003e \"Int\"\r\n      case _:LongType \u003d\u003e \"Long\"\r\n      case _:FloatType \u003d\u003e \"Float\"\r\n      case _:DoubleType \u003d\u003e \"Double\"\r\n      case _:DecimalType \u003d\u003e \"java.math.BigDecimal\"\r\n      case _:StringType \u003d\u003e \"String\"\r\n      case _:BinaryType \u003d\u003e \"Array[Byte]\"\r\n      case _:BooleanType \u003d\u003e \"Boolean\"\r\n      case _:TimestampType \u003d\u003e \"java.sql.Timestamp\"\r\n      case _:DateType \u003d\u003e \"java.sql.Date\"\r\n      case _: ArrayType \u003d\u003e {\r\n        val e \u003d t match { case ArrayType(elementType, _) \u003d\u003e elementType }\r\n        s\"Seq[${defaultTypeConverter(e)}]\"\r\n      }\r\n      case _:MapType \u003d\u003e \"scala.collection.Map\"\r\n      case _:StructType \u003d\u003e \"org.apache.spark.sql.Row\"\r\n      case _ \u003d\u003e \"String\"\r\n    }}\r\n  }\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val s2cc \u003d new Schema2CaseClass\r\nval sh \u003d s2cc.schemaToCaseClass(dfTest.schema, \"schemaClass\")(s2cc.implicits.defaultTypeConverter)\r\nprintln(sh)"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "case class schemaClass (\r\n    instanceId_userId:Option[Int],\r\n    instanceId_objectType:Option[String],\r\n    instanceId_objectId:Option[Int],\r\n    audit_pos:Option[Long],\r\n    audit_clientType:Option[String],\r\n    audit_timestamp:Option[Long],\r\n    audit_timePassed:Option[Long],\r\n    audit_experiment:Option[String],\r\n    audit_resourceType:Option[Long],\r\n    metadata_ownerId:Option[Int],\r\n    metadata_ownerType:Option[String],\r\n    metadata_createdAt:Option[Long],\r\n    metadata_authorId:Option[Int],\r\n    metadata_applicationId:Option[Long],\r\n    metadata_numCompanions:Option[Int],\r\n    metadata_numPhotos:Option[Int],\r\n    metadata_numPolls:Option[Int],\r\n    metadata_numSymbols:Option[Int],\r\n    metadata_numTokens:Option[Int],\r\n    metadata_numVideos:Option[Int],\r\n    metadata_platform:Option[String],\r\n    metadata_totalVideoLength:Option[Int],\r\n    metadata_options:Option[Seq[String]],\r\n    relationsMask:Option[Long],\r\n    userOwnerCounters_USER_FEED_REMOVE:Option[Double],\r\n    userOwnerCounters_USER_PROFILE_VIEW:Option[Double],\r\n    userOwnerCounters_VOTE_POLL:Option[Double],\r\n    userOwnerCounters_USER_SEND_MESSAGE:Option[Double],\r\n    userOwnerCounters_USER_DELETE_MESSAGE:Option[Double],\r\n    userOwnerCounters_USER_INTERNAL_LIKE:Option[Double],\r\n    userOwnerCounters_USER_INTERNAL_UNLIKE:Option[Double],\r\n    userOwnerCounters_USER_STATUS_COMMENT_CREATE:Option[Double],\r\n    userOwnerCounters_PHOTO_COMMENT_CREATE:Option[Double],\r\n    userOwnerCounters_MOVIE_COMMENT_CREATE:Option[Double],\r\n    userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:Option[Double],\r\n    userOwnerCounters_COMMENT_INTERNAL_LIKE:Option[Double],\r\n    userOwnerCounters_USER_FORUM_MESSAGE_CREATE:Option[Double],\r\n    userOwnerCounters_PHOTO_MARK_CREATE:Option[Double],\r\n    userOwnerCounters_PHOTO_VIEW:Option[Double],\r\n    userOwnerCounters_PHOTO_PIN_BATCH_CREATE:Option[Double],\r\n    userOwnerCounters_PHOTO_PIN_UPDATE:Option[Double],\r\n    userOwnerCounters_USER_PRESENT_SEND:Option[Double],\r\n    userOwnerCounters_UNKNOWN:Option[Double],\r\n    userOwnerCounters_CREATE_TOPIC:Option[Double],\r\n    userOwnerCounters_CREATE_IMAGE:Option[Double],\r\n    userOwnerCounters_CREATE_MOVIE:Option[Double],\r\n    userOwnerCounters_CREATE_COMMENT:Option[Double],\r\n    userOwnerCounters_CREATE_LIKE:Option[Double],\r\n    userOwnerCounters_TEXT:Option[Double],\r\n    userOwnerCounters_IMAGE:Option[Double],\r\n    userOwnerCounters_VIDEO:Option[Double],\r\n    ownerUserCounters_USER_FEED_REMOVE:Option[Double],\r\n    ownerUserCounters_USER_PROFILE_VIEW:Option[Double],\r\n    ownerUserCounters_VOTE_POLL:Option[Double],\r\n    ownerUserCounters_USER_SEND_MESSAGE:Option[Double],\r\n    ownerUserCounters_USER_DELETE_MESSAGE:Option[Double],\r\n    ownerUserCounters_USER_INTERNAL_LIKE:Option[Double],\r\n    ownerUserCounters_USER_INTERNAL_UNLIKE:Option[Double],\r\n    ownerUserCounters_USER_STATUS_COMMENT_CREATE:Option[Double],\r\n    ownerUserCounters_PHOTO_COMMENT_CREATE:Option[Double],\r\n    ownerUserCounters_MOVIE_COMMENT_CREATE:Option[Double],\r\n    ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE:Option[Double],\r\n    ownerUserCounters_COMMENT_INTERNAL_LIKE:Option[Double],\r\n    ownerUserCounters_USER_FORUM_MESSAGE_CREATE:Option[Double],\r\n    ownerUserCounters_PHOTO_MARK_CREATE:Option[Double],\r\n    ownerUserCounters_PHOTO_VIEW:Option[Double],\r\n    ownerUserCounters_PHOTO_PIN_BATCH_CREATE:Option[Double],\r\n    ownerUserCounters_PHOTO_PIN_UPDATE:Option[Double],\r\n    ownerUserCounters_USER_PRESENT_SEND:Option[Double],\r\n    ownerUserCounters_UNKNOWN:Option[Double],\r\n    ownerUserCounters_CREATE_TOPIC:Option[Double],\r\n    ownerUserCounters_CREATE_IMAGE:Option[Double],\r\n    ownerUserCounters_CREATE_MOVIE:Option[Double],\r\n    ownerUserCounters_CREATE_COMMENT:Option[Double],\r\n    ownerUserCounters_CREATE_LIKE:Option[Double],\r\n    ownerUserCounters_TEXT:Option[Double],\r\n    ownerUserCounters_IMAGE:Option[Double],\r\n    ownerUserCounters_VIDEO:Option[Double],\r\n    membership_status:Option[String],\r\n    membership_statusUpdateDate:Option[Long],\r\n    membership_joinDate:Option[Long],\r\n    membership_joinRequestDate:Option[Long],\r\n    owner_create_date:Option[Long],\r\n    owner_birth_date:Option[Int],\r\n    owner_gender:Option[Int],\r\n    owner_status:Option[Int],\r\n    owner_ID_country:Option[Long],\r\n    owner_ID_Location:Option[Int],\r\n    owner_is_active:Option[Int],\r\n    owner_is_deleted:Option[Int],\r\n    owner_is_abused:Option[Int],\r\n    owner_is_activated:Option[Int],\r\n    owner_change_datime:Option[Long],\r\n    owner_is_semiactivated:Option[Int],\r\n    owner_region:Option[Int],\r\n    user_create_date:Option[Long],\r\n    user_birth_date:Option[Int],\r\n    user_gender:Option[Int],\r\n    user_status:Option[Int],\r\n    user_ID_country:Option[Long],\r\n    user_ID_Location:Option[Int],\r\n    user_is_active:Option[Int],\r\n    user_is_deleted:Option[Int],\r\n    user_is_abused:Option[Int],\r\n    user_is_activated:Option[Int],\r\n    user_change_datime:Option[Long],\r\n    user_is_semiactivated:Option[Int],\r\n    user_region:Option[Int],\r\n    objectId:Option[Int],\r\n    auditweights_ageMs:Option[Double],\r\n    auditweights_closed:Option[Double],\r\n    auditweights_ctr_gender:Option[Double],\r\n    auditweights_ctr_high:Option[Double],\r\n    auditweights_ctr_negative:Option[Double],\r\n    auditweights_dailyRecency:Option[Double],\r\n    auditweights_feedOwner_RECOMMENDED_GROUP:Option[Double],\r\n    auditweights_feedStats:Option[Double],\r\n    auditweights_friendCommentFeeds:Option[Double],\r\n    auditweights_friendCommenters:Option[Double],\r\n    auditweights_friendLikes:Option[Double],\r\n    auditweights_friendLikes_actors:Option[Double],\r\n    auditweights_hasDetectedText:Option[Double],\r\n    auditweights_hasText:Option[Double],\r\n    auditweights_isPymk:Option[Double],\r\n    auditweights_isRandom:Option[Double],\r\n    auditweights_likersFeedStats_hyper:Option[Double],\r\n    auditweights_likersSvd_prelaunch_hyper:Option[Double],\r\n    auditweights_matrix:Option[Double],\r\n    auditweights_notOriginalPhoto:Option[Double],\r\n    auditweights_numDislikes:Option[Double],\r\n    auditweights_numLikes:Option[Double],\r\n    auditweights_numShows:Option[Double],\r\n    auditweights_onlineVideo:Option[Double],\r\n    auditweights_partAge:Option[Double],\r\n    auditweights_partCtr:Option[Double],\r\n    auditweights_partSvd:Option[Double],\r\n    auditweights_processedVideo:Option[Double],\r\n    auditweights_relationMasks:Option[Double],\r\n    auditweights_source_LIVE_TOP:Option[Double],\r\n    auditweights_source_MOVIE_TOP:Option[Double],\r\n    auditweights_svd_prelaunch:Option[Double],\r\n    auditweights_svd_spark:Option[Double],\r\n    auditweights_userAge:Option[Double],\r\n    auditweights_userOwner_CREATE_COMMENT:Option[Double],\r\n    auditweights_userOwner_CREATE_IMAGE:Option[Double],\r\n    auditweights_userOwner_CREATE_LIKE:Option[Double],\r\n    auditweights_userOwner_IMAGE:Option[Double],\r\n    auditweights_userOwner_MOVIE_COMMENT_CREATE:Option[Double],\r\n    auditweights_userOwner_PHOTO_COMMENT_CREATE:Option[Double],\r\n    auditweights_userOwner_PHOTO_MARK_CREATE:Option[Double],\r\n    auditweights_userOwner_PHOTO_VIEW:Option[Double],\r\n    auditweights_userOwner_TEXT:Option[Double],\r\n    auditweights_userOwner_UNKNOWN:Option[Double],\r\n    auditweights_userOwner_USER_DELETE_MESSAGE:Option[Double],\r\n    auditweights_userOwner_USER_FEED_REMOVE:Option[Double],\r\n    auditweights_userOwner_USER_FORUM_MESSAGE_CREATE:Option[Double],\r\n    auditweights_userOwner_USER_INTERNAL_LIKE:Option[Double],\r\n    auditweights_userOwner_USER_INTERNAL_UNLIKE:Option[Double],\r\n    auditweights_userOwner_USER_PRESENT_SEND:Option[Double],\r\n    auditweights_userOwner_USER_PROFILE_VIEW:Option[Double],\r\n    auditweights_userOwner_USER_SEND_MESSAGE:Option[Double],\r\n    auditweights_userOwner_USER_STATUS_COMMENT_CREATE:Option[Double],\r\n    auditweights_userOwner_VIDEO:Option[Double],\r\n    auditweights_userOwner_VOTE_POLL:Option[Double],\r\n    auditweights_x_ActorsRelations:Option[Long],\r\n    auditweights_likersSvd_spark_hyper:Option[Double],\r\n    auditweights_source_PROMO:Option[Double],\r\n    //date:Option[java.sql.Date]\r\n    date:Option[String]\r\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dataTest \u003d dfTest.as[schemaClass]"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/**\r\n* Add Column Index to dataframe to each row\r\n*/\r\ndef addColumnIndex(df: DataFrame) \u003d {\r\nspark.sqlContext.createDataFrame(\r\n  df.rdd.zipWithIndex.map {\r\n    case (row, index) \u003d\u003e Row.fromSeq(row.toSeq :+ index)\r\n  },\r\n  // Create schema for index column\r\n  StructType(df.schema.fields :+ StructField(\"index\", LongType, false)))\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "implicit val ctx \u003d spark.sqlContext\r\n\r\nval dataStream \u003d MemoryStream[schemaClass]"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val threadSleepTime \u003d 500\r\nval awaitQueryTime \u003d 300000\r\nval idName \u003d \"id\"\r\n\r\nval stream \u003d dataStream.toDF()\r\n\r\nval query \u003d stream\r\n        .writeStream\r\n        .foreachBatch(\r\n            (batchDS: Dataset[_], batchId: Long) \u003d\u003e {\r\n                System.out.println(s\"--- new batch $batchId ---\")\r\n                val prediction \u003d model.transform(batchDS).select(\"prediction\")\r\n                System.out.println(prediction.show)\r\n            }\r\n        )\r\n        //.format(\"console\")\r\n        .start()"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val emul \u003d new Thread(new Runnable() {\r\n    override def run(): Unit \u003d {\r\n        System.out.println(\"--- Thread Start ---\")\r\n        val dataCount \u003d dataTest.count\r\n        val dataWithIndex \u003d addColumnIndex(dataTest.toDF).withColumn(idName, monotonicallyIncreasingId)\r\n        var n \u003d 0\r\n        while (query.isActive \u0026\u0026 n\u003cdataCount) {\r\n            dataStream.addData(\r\n                dataWithIndex\r\n                    .filter(s\"$idName \u003d\u003d $n\")\r\n                    .drop(idName)\r\n                    .as[schemaClass]\r\n                    .first\r\n            )\r\n            n+\u003d1\r\n\r\n            //Thread.sleep(threadSleepTime)\r\n        }\r\n    }\r\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "emul.start()\r\n\r\nquery.awaitTermination(awaitQueryTime)\r\nquery.stop()"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "emul.interrupt()"
    }
  ]
}